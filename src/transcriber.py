"""
Module de transcription utilisant WhisperX avec diarisation intÃ©grÃ©e
"""

import whisperx
from pathlib import Path
from typing import Optional
import torch
import gc
import warnings
import os

# Supprimer les warnings verbeux de torchaudio et pyannote
warnings.filterwarnings("ignore", category=UserWarning, module="torchaudio")
warnings.filterwarnings("ignore", category=UserWarning, module="pyannote")
warnings.filterwarnings("ignore", message=".*torchaudio.*")
warnings.filterwarnings("ignore", message=".*TorchCodec.*")

# Supprimer aussi les logs HuggingFace si trop verbeux
os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'


class Transcriber:
    """GÃ¨re la transcription audio avec WhisperX et diarisation"""

    def __init__(self, config: dict):
        """
        Initialise le transcripteur avec la configuration

        Args:
            config: Dictionnaire de configuration
        """
        self.config = config['transcription']
        self.model = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.compute_type = "float16" if self.device == "cuda" else "int8"

    def charger_modele(self):
        """Charge le modÃ¨le WhisperX"""
        nom_modele = self.config['modele']
        print(f"ğŸ¤– Chargement du modÃ¨le WhisperX '{nom_modele}'...")
        print(f"   ğŸ–¥ï¸  Device : {self.device.upper()}")
        print("   (Cela peut prendre du temps au premier lancement)")

        self.model = whisperx.load_model(
            nom_modele,
            self.device,
            compute_type=self.compute_type
        )
        print("âœ… ModÃ¨le chargÃ©")

    def transcrire(
        self,
        chemin_audio: Path,
        chemin_sortie: Optional[Path] = None,
        detecter_speakers: bool = False,
        token_hf: Optional[str] = None
    ) -> dict:
        """
        Transcrit un fichier audio avec option de diarisation

        Args:
            chemin_audio: Chemin vers le fichier audio
            chemin_sortie: Chemin optionnel pour sauvegarder la transcription
            detecter_speakers: Si True, active la dÃ©tection des speakers
            token_hf: Token HuggingFace (requis si detecter_speakers=True)

        Returns:
            Dictionnaire de rÃ©sultat avec 'texte', 'segments', 'langue'
        """
        if self.model is None:
            self.charger_modele()

        print(f"ğŸ¤ Transcription de {chemin_audio.name}...")

        # Charger l'audio
        audio = whisperx.load_audio(str(chemin_audio))

        # Ã‰tape 1 : Transcription (optimisÃ©e pour le franÃ§ais)
        print("   ğŸ“ Transcription en cours (franÃ§ais)...")

        resultat = self.model.transcribe(
            audio,
            language="fr",  # Toujours franÃ§ais
            batch_size=16
        )

        langue_detectee = "fr"
        print(f"   âœ… Transcription terminÃ©e (langue: {langue_detectee})")

        # Ã‰tape 2 : Alignment pour de meilleurs timestamps (franÃ§ais)
        print("   ğŸ¯ Alignement des timestamps...")
        try:
            model_a, metadata = whisperx.load_align_model(
                language_code="fr",
                device=self.device
            )

            resultat = whisperx.align(
                resultat["segments"],
                model_a,
                metadata,
                audio,
                self.device,
                return_char_alignments=False
            )

            # LibÃ©rer la mÃ©moire
            del model_a
            gc.collect()
            if self.device == "cuda":
                torch.cuda.empty_cache()

            print("   âœ… Alignement terminÃ©")
        except Exception as e:
            print(f"   âš ï¸  Alignement ignorÃ© : {e}")
            # Continuer sans alignment
            resultat = {"segments": resultat["segments"]}

        # Ã‰tape 3 : Diarisation si demandÃ©e
        if detecter_speakers:
            if not token_hf:
                print("   âš ï¸  Token HuggingFace manquant, diarisation ignorÃ©e")
                print("      DÃ©finissez HUGGINGFACE_TOKEN dans .env")
            else:
                resultat = self._ajouter_speakers(
                    str(chemin_audio),  # Passer le chemin du fichier
                    resultat,
                    token_hf
                )

        # Formater le rÃ©sultat
        transcription = self._formater_resultat(resultat, langue_detectee)

        print(f"âœ… Transcription complÃ¨te : {len(transcription['texte'])} caractÃ¨res")
        print(f"   ğŸ“Š {len(transcription['segments'])} segments")

        # Sauvegarder si chemin fourni
        if chemin_sortie:
            self._sauvegarder_transcription(transcription, chemin_sortie)

        return transcription

    def _ajouter_speakers(
        self,
        chemin_audio: str,
        resultat: dict,
        token_hf: str
    ) -> dict:
        """
        Ajoute l'identification des speakers avec Pyannote via WhisperX

        Args:
            chemin_audio: Chemin vers le fichier audio
            resultat: RÃ©sultat de transcription alignÃ©e
            token_hf: Token HuggingFace

        Returns:
            RÃ©sultat enrichi avec speakers
        """
        try:
            print("   ğŸ‘¥ DÃ©tection des speakers...")

            # Utiliser directement pyannote.audio
            from pyannote.audio import Pipeline

            # Charger le pipeline de diarisation
            print("   ğŸ“¦ Chargement du modÃ¨le de diarisation...")
            diarize_model = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1",
                use_auth_token=token_hf
            )
            print("   âœ… ModÃ¨le chargÃ©")

            # Appliquer la diarisation (prend un chemin de fichier)
            print("   ğŸ”„ Analyse audio (1/2) : identification des intervenants...")
            diarize_segments = diarize_model(chemin_audio)
            print("   âœ… Intervenants identifiÃ©s")

            # Utiliser la fonction d'assignment de WhisperX
            print("   ğŸ¯ Analyse audio (2/2) : attribution aux segments...")

            # VÃ©rifier que resultat contient 'segments'
            if 'segments' not in resultat or not resultat['segments']:
                print("   âš ï¸  Pas de segments Ã  traiter pour la diarisation")
                return resultat

            # WhisperX assign_word_speakers attend seulement 2 arguments :
            # - diarize_segments (rÃ©sultat de pyannote)
            # - transcript (le rÃ©sultat de l'alignement avec 'segments' et optionnellement 'word_segments')
            try:
                resultat_diarize = whisperx.assign_word_speakers(
                    diarize_segments,
                    resultat
                )
            except (KeyError, TypeError) as e:
                print(f"   âš ï¸  Erreur d'attribution (segments incompatibles) : {e}")
                print("   ğŸ’¡ Attribution manuelle par segment...")

                # Fallback : attribution manuelle segment par segment
                resultat_diarize = self._attribution_manuelle_speakers(
                    diarize_segments,
                    resultat
                )

            print("   âœ… Attribution terminÃ©e")

            # Compter les speakers
            speakers = set()
            for seg in resultat_diarize.get("segments", []):
                speaker = seg.get("speaker")
                if speaker:
                    speakers.add(speaker)

            if speakers:
                print(f"\n   âœ… Diarisation rÃ©ussie !")
                print(f"   ğŸ‘¥ {len(speakers)} speaker(s) dÃ©tectÃ©(s) : {', '.join(sorted(speakers))}\n")
            else:
                print(f"\n   âš ï¸  Aucun speaker dÃ©tectÃ© (fichier mono-locuteur ?)\n")

            return resultat_diarize

        except ImportError as e:
            print("\n   âš ï¸  pyannote.audio n'est pas installÃ©")
            print("   ğŸ’¡ Installez avec : pip install pyannote.audio\n")
            return resultat

        except Exception as e:
            import traceback
            error_msg = str(e)
            print(f"\n   âš ï¸  Erreur lors de la diarisation : {error_msg}\n")

            if "404" in error_msg or "not found" in error_msg.lower():
                print("   ğŸ’¡ Solutions :")
                print("      1. Acceptez les conditions sur HuggingFace :")
                print("         â€¢ https://huggingface.co/pyannote/speaker-diarization-3.1")
                print("         â€¢ https://huggingface.co/pyannote/segmentation-3.0")
                print("      2. Attendez 5-10 minutes aprÃ¨s l'acceptation")
                print("      3. VÃ©rifiez votre token dans .env\n")
            elif "torch" in error_msg.lower():
                print("   ğŸ’¡ Le problÃ¨me semble liÃ© Ã  PyTorch")
                print("      RÃ©installez : pip install torch torchaudio\n")
            else:
                print("   ğŸ› Trace complÃ¨te :")
                traceback.print_exc()
                print()

            print("   â­ï¸  La transcription continuera sans dÃ©tection de speakers\n")
            return resultat

    def _attribution_manuelle_speakers(
        self,
        diarize_segments,
        resultat: dict
    ) -> dict:
        """
        Attribution manuelle des speakers aux segments (fallback)

        Args:
            diarize_segments: RÃ©sultat de pyannote
            resultat: Transcription alignÃ©e

        Returns:
            RÃ©sultat avec speakers attribuÃ©s
        """
        # Convertir diarize_segments en liste de (start, end, speaker)
        speaker_intervals = []
        for turn, _, speaker in diarize_segments.itertracks(yield_label=True):
            speaker_intervals.append({
                'start': turn.start,
                'end': turn.end,
                'speaker': speaker
            })

        # Attribuer chaque segment au speaker majoritaire
        for segment in resultat['segments']:
            seg_start = segment['start']
            seg_end = segment['end']
            seg_mid = (seg_start + seg_end) / 2

            # Trouver le speaker au milieu du segment
            speaker = None
            max_overlap = 0

            for interval in speaker_intervals:
                # Calculer l'overlap entre le segment et l'intervalle speaker
                overlap_start = max(seg_start, interval['start'])
                overlap_end = min(seg_end, interval['end'])
                overlap = max(0, overlap_end - overlap_start)

                if overlap > max_overlap:
                    max_overlap = overlap
                    speaker = interval['speaker']

            segment['speaker'] = speaker if speaker else "SPEAKER_UNKNOWN"

        return resultat

    def _formater_resultat(self, resultat: dict, langue: str) -> dict:
        """Formate le rÃ©sultat WhisperX au format attendu"""
        segments_formates = []
        texte_complet = []

        for seg in resultat.get("segments", []):
            segment = {
                'debut': seg['start'],
                'fin': seg['end'],
                'texte': seg['text'].strip()
            }

            # Ajouter le speaker si prÃ©sent
            if 'speaker' in seg:
                segment['speaker'] = seg['speaker']

            segments_formates.append(segment)
            texte_complet.append(seg['text'].strip())

        return {
            'texte': ' '.join(texte_complet),
            'langue': langue,
            'segments': segments_formates
        }

    def _sauvegarder_transcription(self, transcription: dict, chemin_sortie: Path):
        """Sauvegarde la transcription dans des fichiers"""
        chemin_sortie.parent.mkdir(parents=True, exist_ok=True)

        # Sauvegarder le texte complet
        fichier_texte = chemin_sortie.with_suffix('.txt')
        with open(fichier_texte, 'w', encoding='utf-8') as f:
            f.write(transcription['texte'])

        # Sauvegarder la version avec timestamps (et speakers si disponibles)
        fichier_timestamps = chemin_sortie.with_name(
            f"{chemin_sortie.stem}_timestamps.txt"
        )
        with open(fichier_timestamps, 'w', encoding='utf-8') as f:
            for seg in transcription['segments']:
                temps_debut = self._formater_temps(seg['debut'])
                temps_fin = self._formater_temps(seg['fin'])

                # Ajouter speaker si disponible
                speaker = seg.get('speaker', '')
                prefix = f"[{speaker}] " if speaker else ""

                f.write(f"[{temps_debut} - {temps_fin}] {prefix}{seg['texte']}\n")

        print(f"ğŸ’¾ Transcription sauvegardÃ©e :")
        print(f"   ğŸ“„ Texte : {fichier_texte.name}")
        print(f"   â±ï¸  Avec timestamps : {fichier_timestamps.name}")

    @staticmethod
    def _formater_temps(secondes: float) -> str:
        """Formate les secondes en MM:SS"""
        minutes = int(secondes // 60)
        secs = int(secondes % 60)
        return f"{minutes:02d}:{secs:02d}"